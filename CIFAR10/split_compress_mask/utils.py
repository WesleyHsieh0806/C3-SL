from matplotlib.pyplot import savefig
from numpy.core.fromnumeric import var
import torch
import torch.nn.functional as F
import sys
import numpy as np


def circular_pad2d(Input, pad: tuple):
    '''
    * Input: (1, channel=1, batch, nof_feautre)
    * Return: padded Input: (1, 1, batch, pad[0]+ nof_feautre + pad[1])
    '''
    output = torch.cat([Input[:, :, :, -pad[0]:], Input,
                        Input[:, :, :, :pad[1]]], dim=3) if pad[0] > 0 else torch.cat([Input,
                                                                                       Input[:, :, :, :pad[1]]], dim=3)
    return output


def circular_pad1d(Input, pad: tuple):
    '''
    * Input: (1, channel=1, batch, nof_feautre)
    * Return: padded Input: (1, 1, batch, pad[0]+ nof_feautre + pad[1])
    '''
    output = torch.cat([Input[:, :, -pad[0]:], Input,
                        Input[:, :, :pad[1]]], dim=2) if pad[0] > 0 else torch.cat([Input,
                                                                                    Input[:, :, :pad[1]]], dim=2)
    return output


def circular_conv(W, Key=None):
    '''
    * Return: (Key, K*W) or K*W if K is not None
    * V:(1, nof_feature) (alread compressed)
    * V = Key * W, where * denotes circular convolution
    * The keys are generated by random normal distribution with mean 0 and variance 1/d
    * With unit norm
    * Key:(Batch size, nof_feature)->(1, 1, Batch size, nof_feature)
    * W:(Batch size, nof_feature)->(1, 1, Batch size, nof_feature)
    '''
    return_key = False
    if Key is None:
        # Generate the key
        mean = 0
        dim = 1
        for i in range(len(W.shape)):
            dim *= W.shape[i]
        std = (1/dim) ** (1/2)
        Key = (torch.randn(W.shape)*std + mean).cuda()
        Key /= torch.norm(Key)
        Key = Key.reshape([1, 1, Key.shape[0], -1])
        return_key = True

    # Circular convolution by circular padding
    # Circular padding: [1,2,3] with pad(2,0) -> [2,3] +[1,2,3] + []
    W = W.reshape([1, 1, W.shape[0], -1])

    # W = [w1 w2 w0 w1 w2] Key = [K2 K1 K0] (flipped)
    # faster than flip() when dim is large
    inverse_idx = torch.arange(Key.shape[3]-1, -1, -1)
    V = F.conv2d(
        circular_pad2d(W, pad=(W.shape[-1]-1, 0)),
        Key[:, :, :, inverse_idx],
        padding=0, stride=1).reshape([1, -1])
    return (Key, V) if return_key else V


def circular_corr(compress_V, Key):
    ''' Circular Correlation
    * Return: KoV (Batch, nof_feature)
    * W = Key o V = Key o (Key*W), where o denotes circular convolution
    * Key:(1, 1, Batch, nof_feature) -> (Batch, 1, nof_feature)
    * compress_V:(1, nof_feature)->(1, 1, nof_feature)
    * W:(1, Batch, nof_feature) -> (Batch, nof_feature)
    '''
    # Circular pad compressV -> [v0 v1 v2] -> [v0 v1 v2 v0 v1]
    # Key = [k0 k1 k2]
    compress_V = compress_V.unsqueeze(dim=0)
    Key = Key.reshape([Key.shape[2], 1, -1])
    W = F.conv1d(circular_pad1d(
        compress_V, pad=(0, compress_V.shape[-1]-1)), Key, stride=1)
    return W.reshape([W.shape[1], -1])


def build_mask(var_np, gamma=0.8):
    '''
    * Mask the features by its variance
    * var_np:(nof_label, feature) the variance of each label 
    *       Type: Numpy array
    * gamma: The proportion of dimension to leave
    * Return: Mask (nof_label, feature)
    *           Type: CUDA Tensor
    '''
    if gamma > 1 or gamma <= 0:
        print("Build_Mask: Gamma should lie in (0,1] !!!")
        sys.exit(1)
    sorted_var = np.sort(var_np, axis=1)
    Dim = var_np.shape[1]

    # threshold:(Label, )
    threshold = sorted_var[:, int((1-gamma)*Dim)]
    mask = (var_np >= threshold[:, None])

    # Transform mask into Tensor
    CUDA_Mask = torch.from_numpy(mask).detach().cuda()
    return CUDA_Mask
